{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:55.165712Z",
     "start_time": "2023-06-05T19:04:54.559718Z"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz as graphviz\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_PROPORTION = 0.7\n",
    "TEST_TO_VALIDATE_RATIO = 0.1\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # load clean data\n",
    "    real_file = open('clean_real.txt', 'r')\n",
    "    fake_file = open('clean_fake.txt', 'r')\n",
    "\n",
    "    # build set of words, and store sentences as list of tokens\n",
    "    real_sentences = [line for line in real_file]\n",
    "    fake_sentences = [line for line in fake_file]\n",
    "    corpus = real_sentences + fake_sentences\n",
    "\n",
    "    # make labels\n",
    "    labels = np.array(['real'] * len(real_sentences) + ['fake'] * len(fake_sentences))\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpus, labels, train_size=TRAIN_PROPORTION)\n",
    "    X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test,\n",
    "                                                              train_size=TEST_TO_VALIDATE_RATIO)\n",
    "\n",
    "    print(f\"Training, validation, test split: ({len(X_train), len(X_test), len(X_validate)})\")\n",
    "\n",
    "    # vectorize the sentences\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_validate = vectorizer.transform(X_validate)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    # return the train, validate, test data and the vectorizer\n",
    "\n",
    "    return (X_train, X_validate, X_test, y_train, y_validate, y_test, vectorizer)\n",
    "\n",
    "\n",
    "def measure_accuracy(test, predicted):\n",
    "    assert len(test) == len(predicted)\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "    for i in range(len(test)):\n",
    "        if test[i] == predicted[i]:\n",
    "            correct += 1\n",
    "        tot += 1\n",
    "    return correct / tot\n",
    "\n",
    "\n",
    "def select_model(x_train, x_validate, X_test, y_train, y_validate, y_test, plot_results=False):\n",
    "    depths = np.arange(50, 300, 50)\n",
    "    criteria = ['gini', 'entropy', 'log_loss']\n",
    "    hyperparams = [(d, c) for d in depths for c in criteria]\n",
    "    val_accuracies = np.zeros((len(criteria), len(depths)))\n",
    "\n",
    "    for i, criterion in enumerate(criteria):\n",
    "        for j, d in enumerate(depths):\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=d, criterion=criterion)\n",
    "            clf = clf.fit(x_train, y_train)\n",
    "\n",
    "            y_validation_prediction = clf.predict(x_validate)\n",
    "\n",
    "            val_accuracies[i, j] = measure_accuracy(y_validate, y_validation_prediction)\n",
    "\n",
    "            print(\n",
    "                f\"Depth {d:3} with {criterion:8} criterion had validation accuracy {measure_accuracy(y_validate, y_validation_prediction):0.5f} \")\n",
    "\n",
    "    best_ind = np.argmax(val_accuracies)\n",
    "\n",
    "    return hyperparams[best_ind]\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # for criterion in criteria:\n",
    "    #     y_acc = []\n",
    "    #     plt.scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading random splits of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:55.203366Z",
     "start_time": "2023-06-05T19:04:55.166659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, test split: ((2286, 98, 882))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, X_test, y_train, y_validate, y_test, vectorizer = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:55.967460Z",
     "start_time": "2023-06-05T19:04:55.200998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth  50 with gini     criterion had validation accuracy 0.75510 \n",
      "Depth 100 with gini     criterion had validation accuracy 0.75964 \n",
      "Depth 150 with gini     criterion had validation accuracy 0.76190 \n",
      "Depth 200 with gini     criterion had validation accuracy 0.75624 \n",
      "Depth 250 with gini     criterion had validation accuracy 0.75964 \n",
      "Depth  50 with entropy  criterion had validation accuracy 0.76190 \n",
      "Depth 100 with entropy  criterion had validation accuracy 0.74036 \n",
      "Depth 150 with entropy  criterion had validation accuracy 0.76417 \n",
      "Depth 200 with entropy  criterion had validation accuracy 0.75964 \n",
      "Depth 250 with entropy  criterion had validation accuracy 0.76077 \n",
      "Depth  50 with log_loss criterion had validation accuracy 0.76871 \n",
      "Depth 100 with log_loss criterion had validation accuracy 0.75397 \n",
      "Depth 150 with log_loss criterion had validation accuracy 0.75510 \n",
      "Depth 200 with log_loss criterion had validation accuracy 0.76417 \n",
      "Depth 250 with log_loss criterion had validation accuracy 0.75964 \n"
     ]
    }
   ],
   "source": [
    "depth, criterion = select_model(X_train, X_validate, X_test, y_train, y_validate, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:56.031426Z",
     "start_time": "2023-06-05T19:04:55.968582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A model trained on the best hyperparameters (depth=200, criterion=entropy) had test accuracy 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "# train a model with the best hyperparameters\n",
    "clf = tree.DecisionTreeClassifier(max_depth=depth, criterion=criterion)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# report its accuracy on the test dataset\n",
    "\n",
    "y_test_prediction = clf.predict(X_test)\n",
    "acc = measure_accuracy(y_test, y_test_prediction)\n",
    "\n",
    "print(f\"\\nA model trained on the best hyperparameters (depth={depth}, criterion={criterion}) had test accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:05:02.861542Z",
     "start_time": "2023-06-05T19:05:02.852155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04648751025563547\n",
      "0.04574154251227758\n",
      "0.049144966251183364\n",
      "0.039271303507010136\n",
      "0.010273031057654092\n"
     ]
    }
   ],
   "source": [
    "def calc_entropy(prob_array: np.array):\n",
    "    return -sum(prob_array * np.log2(prob_array))\n",
    "\n",
    "def calc_expectation(var_vals: np.array, var_probs: np.array):\n",
    "    return sum(var_vals * var_probs)\n",
    "\n",
    "\n",
    "def compute_information_gain(X_train, y_train, feature, threshold, vectorizer):\n",
    "\n",
    "    feature_arr = vectorizer.get_feature_names_out()\n",
    "    feature_ind = np.where(feature_arr == feature)[0][0]\n",
    "\n",
    "    above_t = np.transpose((X_train[:,feature_ind] >= threshold).toarray())[0] # contains True if the feature for a datapoint is above the threshold.\n",
    "    below_t = np.logical_not(above_t) # contains True if the feature for a datapoint is below the threshold.\n",
    "\n",
    "    reals = y_train == \"real\"\n",
    "    fakes = y_train == \"fake\"\n",
    "\n",
    "    counts: np.array = np.array([\n",
    "        [np.logical_and(below_t, fakes).sum(), np.logical_and(below_t, reals).sum()],    # prob(Y = false)    prob(Y = real)  when feature <  T\n",
    "        [np.logical_and(above_t, fakes).sum(), np.logical_and(above_t, reals).sum()]   # prob(Y = false)    prob(Y = real)  when feature >= T\n",
    "    ])\n",
    "    probs: np.array = counts / len(y_train)\n",
    "\n",
    "    x_probs: np.array = probs.sum(axis=1)\n",
    "    y_probs: np.array = probs.sum(axis=0)\n",
    "\n",
    "    probs_y_given_x = probs / x_probs[:, None]\n",
    "\n",
    "    entropy_y = calc_entropy(y_probs)\n",
    "\n",
    "    conditional_entropies = [calc_entropy(probs_y_given_x[i]) for i in range(len(x_probs))]\n",
    "\n",
    "    expected_conditional_entropy = calc_expectation(conditional_entropies, x_probs)\n",
    "\n",
    "    inf_gain = entropy_y - expected_conditional_entropy\n",
    "\n",
    "    return inf_gain\n",
    "\n",
    "print(compute_information_gain(X_train, y_train, \"donald\", 0.5, vectorizer))\n",
    "print(compute_information_gain(X_train, y_train, \"trumps\", 0.5, vectorizer))\n",
    "print(compute_information_gain(X_train, y_train, \"the\", 0.5, vectorizer))\n",
    "print(compute_information_gain(X_train, y_train, \"hillary\", 0.5, vectorizer))\n",
    "print(compute_information_gain(X_train, y_train, \"and\", 0.5, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:05:04.415392Z",
     "start_time": "2023-06-05T19:05:04.231021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'figures/iris.png'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf,\n",
    "                                    out_file=None,\n",
    "                                    feature_names=vectorizer.get_feature_names_out(),\n",
    "                                    max_depth=2,\n",
    "                                    filled=True,\n",
    "                                    rounded=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename=\"iris\", directory=\"figures\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:56.418509Z",
     "start_time": "2023-06-05T19:04:56.417879Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:56.421424Z",
     "start_time": "2023-06-05T19:04:56.419814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T19:04:56.421673Z",
     "start_time": "2023-06-05T19:04:56.419962Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
